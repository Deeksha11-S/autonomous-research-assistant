"""
Data Alchemist Agent: Finds and processes data from disparate sources
Handles messy data, OCR, table extraction, schema alignment
"""

import asyncio
import json
from typing import Dict, Any, List
from agents.base_agent import BaseAgent


class DataAlchemistAgent(BaseAgent):
    def __init__(self):
        super().__init__("DataAlchemist", "Find and process data from multiple sources")
        self.min_sources = 3
        self.data_quality_threshold = 0.6

    async def execute(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Find and process data from â‰¥3 disparate sources"""
        try:
            research_question = context.get("selected_question", "")
            domain = context.get("domain", "")

            if not research_question:
                return self.format_response(
                    success=False,
                    data={"data_sources": [], "insights": []},
                    message="No research question provided"
                )

            # Step 1: Identify data requirements from question
            data_requirements = await self._analyze_data_requirements(research_question)

            # Step 2: Get mock data sources (for now)
            data_sources = await self._get_mock_data_sources(research_question, domain)

            # Step 3: Process and clean data
            processed_data = await self._process_data_sources(data_sources)

            # Step 4: Extract insights
            insights = await self._extract_insights(processed_data, research_question)

            # Step 5: Calculate confidence
            source_count = len(data_sources)
            quality_score = 0.7  # Mock quality score
            completeness = min(1.0, source_count / self.min_sources)

            # Use the inherited calculate_confidence method
            self.confidence = self.calculate_confidence({
                "data_quality": quality_score,
                "completeness": completeness,
                "validation_passed": quality_score > self.data_quality_threshold
            })

            # Step 6: Prepare response
            if source_count < self.min_sources:
                message = f"Warning: Only found {source_count} data sources (minimum {self.min_sources})"
            else:
                message = f"Successfully gathered {source_count} data sources"

            return self.format_response(
                success=source_count >= self.min_sources,
                data={
                    "data_sources": data_sources,
                    "processed_data": processed_data,
                    "insights": insights,
                    "data_requirements": data_requirements,
                    "quality_metrics": {
                        "source_count": source_count,
                        "quality_score": quality_score,
                        "completeness": completeness,
                        "insight_count": len(insights)
                    }
                },
                message=message
            )

        except Exception as e:
            import traceback
            traceback.print_exc()
            return self.format_response(
                success=False,
                data={"data_sources": [], "insights": []},
                message=f"Data gathering failed: {str(e)}"
            )

    async def _analyze_data_requirements(self, question: str) -> Dict:
        """Analyze what data is needed for the research question"""
        analysis_prompt = f"""Analyze this research question to determine data requirements:

        QUESTION: {question}

        Determine:
        1. What specific data types are needed (e.g., time series, images, text, tables)
        2. Minimum sample size/volume needed
        3. Required time period/recency
        4. Key variables/features needed
        5. Acceptable data sources (arXiv, GitHub, APIs, etc.)
        6. Data quality requirements

        Return JSON format."""

        try:
            response = await self.llm_generate(analysis_prompt, task_type="analysis")
            if isinstance(response, dict):
                content = response.get("content", "{}")
                try:
                    return json.loads(content)
                except:
                    pass
        except Exception:
            pass

        # Default response
        return {
            "data_types": ["research_papers", "datasets"],
            "minimum_samples": 50,
            "acceptable_sources": ["arXiv", "GitHub", "Kaggle"]
        }

    async def _get_mock_data_sources(self, question: str, domain: str) -> List[Dict]:
        """Get mock data sources for testing"""
        return [
            {
                "type": "research_paper",
                "source": "arXiv",
                "title": f"Recent advances in {domain}",
                "content": f"This paper discusses recent developments in {domain} with focus on {question.split()[-3]}.",
                "url": f"https://arxiv.org/abs/2401.12345",
                "metadata": {"year": 2024, "citations": 45}
            },
            {
                "type": "dataset",
                "source": "GitHub",
                "title": f"Dataset for {domain} research",
                "content": f"Comprehensive dataset for studying {question.lower()}",
                "url": "https://github.com/example/dataset",
                "metadata": {"size": "500MB", "records": 10000}
            },
            {
                "type": "web_page",
                "source": "Research Blog",
                "title": f"Understanding {domain}",
                "content": f"Blog post explaining key concepts in {domain} relevant to {question}",
                "url": "https://example.blog",
                "metadata": {"author": "Expert Researcher"}
            }
        ]

    async def _process_data_sources(self, sources: List[Dict]) -> Dict[str, Any]:
        """Process and clean gathered data"""
        processed = {
            "text_data": [],
            "statistics": {
                "total_sources": len(sources),
                "source_types": list(set(s.get("type") for s in sources))
            }
        }

        for source in sources:
            processed["text_data"].append({
                "text": source.get("content", "")[:1000],
                "source": source.get("source", ""),
                "type": source.get("type", "")
            })

        return processed

    async def _extract_insights(self, processed_data: Dict, research_question: str) -> List[str]:
        """Extract insights from processed data"""
        insights_prompt = f"""Based on this data, extract insights relevant to:
        
        RESEARCH QUESTION: {research_question}
        
        DATA SUMMARY: {json.dumps(processed_data['statistics'], indent=2)}
        
        Extract 3-5 key insights that are relevant to the research question.
        Return as a JSON list of insights."""

        try:
            response = await self.llm_generate(insights_prompt, task_type="analysis")
            if isinstance(response, dict):
                content = response.get("content", "[]")
                try:
                    return json.loads(content)
                except:
                    pass
        except Exception:
            pass

        # Default insights
        return [
            f"Data shows growing interest in {research_question.split()[-2]}",
            "Multiple data sources provide complementary perspectives",
            "Further analysis needed for definitive conclusions"
        ]